# ğŸ“˜ AI ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ì‹œìŠ¤í…œ - í”„ë¡œì íŠ¸ ìˆ˜í–‰ ë§¤ë‰´ì–¼

> **ëª©í‘œ**: ê°€ì§œ ìŒì„± íƒì§€ + ë²•ì  ê·¼ê±° RAG ê¸°ë°˜ ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ë§ ë° LLMOps ìµœì í™”

---

## ğŸ“‹ ëª©ì°¨

1. [Phase 1: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„](#phase-1-í™˜ê²½-ì„¤ì •-ë°-ë°ì´í„°-ì¤€ë¹„-1-2ì¼)
2. [Phase 2: íƒì§€ ëª¨ë¸ êµ¬ì¶•](#phase-2-íƒì§€-ëª¨ë¸-êµ¬ì¶•-3-5ì¼)
3. [Phase 3: RAG ì‹œìŠ¤í…œ êµ¬ì¶•](#phase-3-rag-ì‹œìŠ¤í…œ-êµ¬ì¶•-2-3ì¼)
4. [Phase 4: API ì„œë²„ ë° í”„ë¡ íŠ¸ì—”ë“œ](#phase-4-api-ì„œë²„-ë°-í”„ë¡ íŠ¸ì—”ë“œ-2-3ì¼)
5. [Phase 5: LLMOps ëª¨ë‹ˆí„°ë§](#phase-5-llmops-ëª¨ë‹ˆí„°ë§-2-3ì¼)
6. [Phase 6: í‰ê°€ ë° ìµœì í™”](#phase-6-í‰ê°€-ë°-ìµœì í™”-3-5ì¼)
7. [Phase 7: ë°°í¬ ë° ë¬¸ì„œí™”](#phase-7-ë°°í¬-ë°-ë¬¸ì„œí™”-1-2ì¼)

---

## Phase 1: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„ (1-2ì¼)

### 1.1 ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì´ë™
cd /home/dhc99/voice-phishing-detection

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 4. GPU í™•ì¸ (ì„ íƒì‚¬í•­)
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

### 1.2 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

#### ASVspoof 2021 (í•„ìˆ˜ - ë©”ì¸ ë²¤ì¹˜ë§ˆí¬)
```bash
# 1. ê³µì‹ ì‚¬ì´íŠ¸ ë“±ë¡: https://www.asvspoof.org/index2021.html
# 2. ë‹¤ìš´ë¡œë“œ ë§í¬ ì´ë©”ì¼ ìˆ˜ì‹ 
# 3. LA (Logical Access) íŠ¸ë™ ë‹¤ìš´ë¡œë“œ

mkdir -p data/audio/asvspoof2021
cd data/audio/asvspoof2021

# ë‹¤ìš´ë¡œë“œ (ë°›ì€ ë§í¬ë¡œ ëŒ€ì²´)
wget <YOUR_DOWNLOAD_LINK>/LA.zip
unzip LA.zip
```

#### MLAAD (ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸ìš©)
```bash
# Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ
pip install datasets

python << 'EOF'
from datasets import load_dataset
ds = load_dataset("Habs/MLAAD", split="train")
ds.save_to_disk("data/audio/mlaad")
print(f"âœ… MLAAD ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(ds)} samples")
EOF
```

#### WaveFake (ì¶”ê°€ í‰ê°€ìš©)
```bash
mkdir -p data/audio/wavefake
cd data/audio/wavefake
wget https://zenodo.org/record/5642694/files/generated_audio.zip
unzip generated_audio.zip
```

### 1.3 ë°ì´í„° êµ¬ì¡° í™•ì¸

```bash
# ë°ì´í„° êµ¬ì¡° í™•ì¸
tree data/audio -L 2

# ì˜ˆìƒ êµ¬ì¡°:
# data/audio/
# â”œâ”€â”€ asvspoof2021/
# â”‚   â”œâ”€â”€ LA/
# â”‚   â”‚   â”œâ”€â”€ ASVspoof2021_LA_train/
# â”‚   â”‚   â”œâ”€â”€ ASVspoof2021_LA_dev/
# â”‚   â”‚   â””â”€â”€ ASVspoof2021_LA_eval/
# â”œâ”€â”€ mlaad/
# â””â”€â”€ wavefake/
```

### âœ… Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ê°€ìƒí™˜ê²½ í™œì„±í™” ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
- [ ] `.env` íŒŒì¼ API í‚¤ ì„¤ì • ì™„ë£Œ
- [ ] ASVspoof 2021 LA ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
- [ ] ë°ì´í„° ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

---

## Phase 2: íƒì§€ ëª¨ë¸ êµ¬ì¶• (3-5ì¼)

### 2.1 ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
# notebooks/01_data_preprocessing.ipynb ìƒì„±

import librosa
import numpy as np
from pathlib import Path
import pandas as pd

# ASVspoof í”„ë¡œí† ì½œ íŒŒì¼ ë¡œë“œ
def load_protocol(protocol_path):
    """ASVspoof í”„ë¡œí† ì½œ íŒŒì¼ íŒŒì‹±"""
    data = []
    with open(protocol_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            data.append({
                'speaker_id': parts[0],
                'audio_file': parts[1],
                'system_id': parts[3],  # bonafide or spoof system
                'label': parts[4]  # bonafide / spoof
            })
    return pd.DataFrame(data)

# ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
def preprocess_audio(audio_path, sr=16000, max_duration=4):
    """ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì •ê·œí™”"""
    waveform, _ = librosa.load(audio_path, sr=sr)
    
    # ê¸¸ì´ ë§ì¶”ê¸°
    max_samples = sr * max_duration
    if len(waveform) > max_samples:
        waveform = waveform[:max_samples]
    else:
        waveform = np.pad(waveform, (0, max_samples - len(waveform)))
    
    # ì •ê·œí™”
    waveform = waveform / (np.max(np.abs(waveform)) + 1e-8)
    
    return waveform

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class ASVspoofDataset:
    def __init__(self, audio_dir, protocol_path):
        self.audio_dir = Path(audio_dir)
        self.protocol = load_protocol(protocol_path)
        
    def __len__(self):
        return len(self.protocol)
    
    def __getitem__(self, idx):
        row = self.protocol.iloc[idx]
        audio_path = self.audio_dir / f"{row['audio_file']}.flac"
        waveform = preprocess_audio(audio_path)
        label = 0 if row['label'] == 'bonafide' else 1
        return waveform, label
```

### 2.2 ëª¨ë¸ í•™ìŠµ (AASIST ë˜ëŠ” RawNet2)

```python
# notebooks/02_model_training.ipynb ìƒì„±

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from src.detection.detector import RawNet2, AASIST

# ì„¤ì •
CONFIG = {
    'model': 'rawnet2',  # 'rawnet2' or 'aasist'
    'batch_size': 32,
    'learning_rate': 1e-4,
    'epochs': 50,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

# ëª¨ë¸ ì´ˆê¸°í™”
if CONFIG['model'] == 'rawnet2':
    model = RawNet2(num_classes=2)
else:
    model = AASIST(num_classes=2)

model = model.to(CONFIG['device'])

# í•™ìŠµ
optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
criterion = nn.CrossEntropyLoss()

def train_epoch(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (audio, labels) in enumerate(dataloader):
        audio = audio.to(CONFIG['device'])
        labels = labels.to(CONFIG['device'])
        
        optimizer.zero_grad()
        outputs = model(audio)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        if batch_idx % 100 == 0:
            print(f"Batch {batch_idx}, Loss: {loss.item():.4f}")
    
    return total_loss / len(dataloader), correct / total

# í•™ìŠµ ë£¨í”„
for epoch in range(CONFIG['epochs']):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)
    val_loss, val_acc = evaluate(model, val_loader, criterion)
    
    print(f"Epoch {epoch+1}/{CONFIG['epochs']}")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if val_acc > best_acc:
        torch.save(model.state_dict(), f"models/checkpoints/{CONFIG['model']}_best.pt")
        best_acc = val_acc
```

### 2.3 ëª¨ë¸ í‰ê°€ (EER, min-tDCF)

```python
# notebooks/03_model_evaluation.ipynb ìƒì„±

from sklearn.metrics import roc_curve, roc_auc_score
import numpy as np

def compute_eer(y_true, y_scores):
    """Equal Error Rate ê³„ì‚°"""
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    fnr = 1 - tpr
    
    # EER: FPR = FNR ì§€ì 
    eer_idx = np.nanargmin(np.abs(fpr - fnr))
    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2
    eer_threshold = thresholds[eer_idx]
    
    return eer, eer_threshold

def compute_min_tdcf(y_true, y_scores, Pspoof=0.05, Cmiss=1, Cfa=10):
    """min t-DCF ê³„ì‚° (ASVspoof í‘œì¤€)"""
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    fnr = 1 - tpr
    
    # t-DCF ê³„ì‚°
    Ptar = 1 - Pspoof
    dcf = Cmiss * fnr * Ptar + Cfa * fpr * Pspoof
    min_tdcf = np.min(dcf)
    
    return min_tdcf

# í‰ê°€ ì‹¤í–‰
model.eval()
all_scores = []
all_labels = []

with torch.no_grad():
    for audio, labels in test_loader:
        audio = audio.to(CONFIG['device'])
        outputs = model(audio)
        probs = torch.softmax(outputs, dim=1)[:, 1]  # spoof í™•ë¥ 
        
        all_scores.extend(probs.cpu().numpy())
        all_labels.extend(labels.numpy())

# ë©”íŠ¸ë¦­ ê³„ì‚°
eer, threshold = compute_eer(all_labels, all_scores)
min_tdcf = compute_min_tdcf(all_labels, all_scores)
auc = roc_auc_score(all_labels, all_scores)

print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
print(f"  EER: {eer*100:.2f}%")
print(f"  min t-DCF: {min_tdcf:.4f}")
print(f"  AUC: {auc:.4f}")
```

### âœ… Phase 2 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] RawNet2 ë˜ëŠ” AASIST ëª¨ë¸ í•™ìŠµ
- [ ] ASVspoof 2021-LA dev set í‰ê°€
- [ ] EER < 5% ë‹¬ì„± í™•ì¸
- [ ] ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (`models/checkpoints/`)

---

## Phase 3: RAG ì‹œìŠ¤í…œ êµ¬ì¶• (2-3ì¼)

### 3.1 ë²•ë¥  ë¬¸ì„œ ìˆ˜ì§‘ ë° ì¸ë±ì‹±

```python
# notebooks/04_rag_setup.ipynb ìƒì„±

from src.rag.legal_rag import LegalDocumentLoader, VectorStore

# 1. ë²•ë¥  ë¬¸ì„œ ë¡œë“œ
loader = LegalDocumentLoader(docs_path="data/legal_docs")
documents = loader.get_documents()

print(f"ğŸ“š ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
for doc in documents:
    print(f"  - {doc.metadata['title']}")

# 2. ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
vector_store = VectorStore(
    embedding_model="text-embedding-3-small",
    index_path="data/vectors"
)

# ì¸ë±ìŠ¤ ë¹Œë“œ
vector_store.build_index(documents)
vector_store.save_index("legal_docs")

print("âœ… ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
```

### 3.2 RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

```python
from src.rag.legal_rag import LegalRAG, create_rag_system

# RAG ì‹œìŠ¤í…œ ìƒì„±
rag = create_rag_system(
    docs_path="data/legal_docs",
    vector_path="data/vectors",
    llm_model="gpt-4o-mini"
)

# í…ŒìŠ¤íŠ¸ ì§ˆì˜
test_questions = [
    "ë³´ì´ìŠ¤í”¼ì‹± í”¼í•´ë¥¼ ë‹¹í–ˆì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
    "ë³´ì´ìŠ¤í”¼ì‹± ì‚¬ê¸°ì£„ì˜ í˜•ëŸ‰ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "ì§€ê¸‰ì •ì§€ ì‹ ì²­ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
    "í†µì¥ ëª…ì˜ë¥¼ ë¹Œë ¤ì¤€ ê²½ìš° ì²˜ë²Œë°›ë‚˜ìš”?"
]

for question in test_questions:
    print(f"\nâ“ ì§ˆë¬¸: {question}")
    response = rag.query(question, risk_level="HIGH")
    print(f"ğŸ“œ ë‹µë³€: {response.answer[:200]}...")
    print(f"ğŸ“š ì°¸ì¡°: {[s['title'] for s in response.sources]}")
    print(f"â±ï¸ ì‘ë‹µì‹œê°„: {response.total_latency_ms:.0f}ms")
```

### 3.3 ì¶”ê°€ ë²•ë ¹ ë¬¸ì„œ í™•ì¥ (ì„ íƒ)

```bash
# ë²•ì œì²˜ì—ì„œ ì¶”ê°€ ë²•ë ¹ ìˆ˜ì§‘
# https://www.law.go.kr ì—ì„œ ë‹¤ìŒ ë²•ë ¹ ê²€ìƒ‰í•˜ì—¬ JSON ì¶”ê°€:
# - ì •ë³´í†µì‹ ë§ ì´ìš©ì´‰ì§„ ë° ì •ë³´ë³´í˜¸ ë“±ì— ê´€í•œ ë²•ë¥ 
# - ê°œì¸ì •ë³´ ë³´í˜¸ë²•
# - ê¸ˆìœµì‹¤ëª…ê±°ë˜ ë° ë¹„ë°€ë³´ì¥ì— ê´€í•œ ë²•ë¥ 
```

### âœ… Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë²•ë¥  ë¬¸ì„œ JSON íŒŒì¼ ìƒì„± ì™„ë£Œ
- [ ] ë²¡í„° ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ
- [ ] RAG ì§ˆì˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] ë²•ë ¹ ì¸ìš© ì •í™•ì„± í™•ì¸

---

## Phase 4: API ì„œë²„ ë° í”„ë¡ íŠ¸ì—”ë“œ (2-3ì¼)

### 4.1 FastAPI ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

```bash
# 1. API ì„œë²„ ì‹¤í–‰
cd /home/dhc99/voice-phishing-detection
source venv/bin/activate
uvicorn src.api.main:app --reload --port 8000

# 2. ìƒˆ í„°ë¯¸ë„ì—ì„œ API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/health

# 3. Swagger UI í™•ì¸
# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000/docs ì ‘ì†
```

### 4.2 API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```python
# notebooks/05_api_test.ipynb ìƒì„±

import requests

API_URL = "http://localhost:8000"

# 1. í—¬ìŠ¤ì²´í¬
response = requests.get(f"{API_URL}/health")
print(f"Health: {response.json()}")

# 2. ìŒì„± íƒì§€ í…ŒìŠ¤íŠ¸
with open("data/audio/test_sample.wav", "rb") as f:
    files = {"file": ("test.wav", f, "audio/wav")}
    response = requests.post(
        f"{API_URL}/api/v1/detect",
        files=files,
        params={"model_type": "aasist"}
    )
    
result = response.json()
print(f"íƒì§€ ê²°ê³¼:")
print(f"  ê°€ì§œ í™•ë¥ : {result['fake_probability']:.1%}")
print(f"  ë¦¬ìŠ¤í¬: {result['risk_level_label']}")

# 3. ë²•ë¥  ì§ˆì˜ í…ŒìŠ¤íŠ¸
response = requests.post(
    f"{API_URL}/api/v1/legal-query",
    json={
        "question": "ë³´ì´ìŠ¤í”¼ì‹± ì‹ ê³ ëŠ” ì–´ë””ì— í•˜ë‚˜ìš”?",
        "risk_level": "HIGH"
    }
)
print(f"\në²•ë¥  ë‹µë³€:\n{response.json()['answer'][:300]}...")
```

### 4.3 Streamlit í”„ë¡ íŠ¸ì—”ë“œ ì‹¤í–‰

```bash
# 1. Streamlit ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
streamlit run frontend/app.py --server.port 8501

# 2. ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
# http://localhost:8501
```

### 4.4 í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```bash
# API + Streamlit ë™ì‹œ ì‹¤í–‰
./scripts/run.sh
```

### âœ… Phase 4 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] FastAPI ì„œë²„ ì •ìƒ ì‹¤í–‰
- [ ] `/api/v1/detect` ì—”ë“œí¬ì¸íŠ¸ ë™ì‘ í™•ì¸
- [ ] `/api/v1/legal-query` ì—”ë“œí¬ì¸íŠ¸ ë™ì‘ í™•ì¸
- [ ] Streamlit UI ì •ìƒ í‘œì‹œ
- [ ] ìŒì„± ì—…ë¡œë“œ â†’ ë¶„ì„ â†’ ê²°ê³¼ í‘œì‹œ í”Œë¡œìš° í™•ì¸

---

## Phase 5: LLMOps ëª¨ë‹ˆí„°ë§ (2-3ì¼)

### 5.1 Langfuse ëŒ€ì‹œë³´ë“œ ì„¤ì •

```bash
# 1. Langfuse í´ë¼ìš°ë“œ ì ‘ì†
# https://cloud.langfuse.com

# 2. í”„ë¡œì íŠ¸ ìƒì„± (ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ)

# 3. API í‚¤ í™•ì¸ (.envì— ì´ë¯¸ ì„¤ì •ë¨)
cat .env | grep LANGFUSE
```

### 5.2 Langfuse íŠ¸ë ˆì´ì‹± í™•ì¸

```python
# notebooks/06_langfuse_monitoring.ipynb ìƒì„±

from langfuse import Langfuse

# Langfuse í´ë¼ì´ì–¸íŠ¸
langfuse = Langfuse()

# íŠ¸ë ˆì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
trace = langfuse.trace(
    name="test-detection-flow",
    user_id="test-user",
    metadata={"test": True}
)

# ìŠ¤íŒ¬ ì¶”ê°€
span = trace.span(
    name="audio-preprocessing",
    input={"audio_file": "test.wav"},
    output={"duration_ms": 1000}
)

# Generation ë¡œê¹…
trace.generation(
    name="legal-response",
    model="gpt-4o-mini",
    input="ë³´ì´ìŠ¤í”¼ì‹± ì‹ ê³  ë°©ë²•",
    output="ê²½ì°°ì²­ 112 ë˜ëŠ” ê¸ˆìœµê°ë…ì› 1332ë¡œ ì‹ ê³ í•˜ì„¸ìš”.",
    usage={"input": 50, "output": 100}
)

langfuse.flush()
print("âœ… Langfuse íŠ¸ë ˆì´ì‹± ì„±ê³µ!")
print("   ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸: https://cloud.langfuse.com")
```

### 5.3 í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ ë° A/B í…ŒìŠ¤íŠ¸

```python
# í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ ì„±ëŠ¥ ë¹„êµ

from src.config import prompts
import time

prompt_versions = {
    "v1": prompts.DETECTION_SUMMARY_V1,
    "v2": prompts.DETECTION_SUMMARY_V2
}

results = []

for version, prompt_template in prompt_versions.items():
    start = time.time()
    
    # LLM í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„)
    response = call_llm(prompt_template.format(
        model_name="AASIST",
        fake_probability=0.85,
        risk_level="HIGH",
        acoustic_anomalies="ë†’ì€ ìŠ¤í™íŠ¸ëŸ¼ í‰íƒ„ë„",
        watermark_detected="ì—†ìŒ"
    ))
    
    latency = (time.time() - start) * 1000
    
    results.append({
        "version": version,
        "latency_ms": latency,
        "tokens": response.usage.total_tokens,
        "response_length": len(response.content)
    })
    
    # Langfuseì— ê¸°ë¡
    trace.generation(
        name=f"summary-{version}",
        model="gpt-4o-mini",
        metadata={"prompt_version": version}
    )

# ë¹„êµ ê²°ê³¼
import pandas as pd
df = pd.DataFrame(results)
print(df.to_markdown())
```

### 5.4 ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ í™•ì¸

```python
# API ë©”íŠ¸ë¦­ ì¡°íšŒ
response = requests.get(f"{API_URL}/api/v1/metrics")
metrics = response.json()

print("ğŸ“Š ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­:")
print(f"  ì´ ìš”ì²­: {metrics['requests']['total']}")
print(f"  ì„±ê³µë¥ : {metrics['requests']['success_rate']:.1%}")
print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {metrics['latency']['total']['mean']:.0f}ms")
print(f"  ë¦¬ìŠ¤í¬ ë¶„í¬: {metrics['risk_distribution']}")
```

### âœ… Phase 5 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Langfuse ëŒ€ì‹œë³´ë“œ ì ‘ì† í™•ì¸
- [ ] API ìš”ì²­ íŠ¸ë ˆì´ìŠ¤ ê¸°ë¡ í™•ì¸
- [ ] LLM í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  í™•ì¸
- [ ] í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ
- [ ] ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ ë°ì´í„° í™•ì¸

---

## Phase 6: í‰ê°€ ë° ìµœì í™” (3-5ì¼)

### 6.1 íƒì§€ ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
# notebooks/07_benchmark.ipynb ìƒì„±

import pandas as pd

# ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
models = ['aasist', 'rawnet2', 'ecapa']
datasets = ['asvspoof2021_la', 'mlaad', 'wavefake']

results = []
for model in models:
    for dataset in datasets:
        eer, min_tdcf, rtf = evaluate_model(model, dataset)
        results.append({
            'model': model,
            'dataset': dataset,
            'EER': eer,
            'min_tDCF': min_tdcf,
            'RTF': rtf  # Real-Time Factor
        })

df = pd.DataFrame(results)
print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(df.pivot_table(index='model', columns='dataset', values='EER'))
```

### 6.2 Latency ìµœì í™”

```python
# ë‹¨ê³„ë³„ Latency ë¶„í•´

import time

def profile_pipeline(audio_path, question):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í”„ë¡œíŒŒì¼ë§"""
    timings = {}
    
    # 1. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
    start = time.time()
    waveform = preprocess_audio(audio_path)
    timings['preprocess'] = (time.time() - start) * 1000
    
    # 2. íƒì§€ ì¶”ë¡ 
    start = time.time()
    detection_result = detector.predict(audio_path)
    timings['detection'] = (time.time() - start) * 1000
    
    # 3. ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ë§
    start = time.time()
    risk_result = scorer.assess_risk(detection_result.fake_probability)
    timings['scoring'] = (time.time() - start) * 1000
    
    # 4. LLM ìš”ì•½
    start = time.time()
    summary = generate_summary(detection_result, risk_result)
    timings['llm_summary'] = (time.time() - start) * 1000
    
    # 5. RAG ê²€ìƒ‰ + ìƒì„±
    start = time.time()
    rag_response = rag.query(question, risk_result.risk_level.value)
    timings['rag'] = (time.time() - start) * 1000
    
    timings['total'] = sum(timings.values())
    
    return timings

# í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
timings = profile_pipeline("test.wav", "ì‹ ê³  ë°©ë²•ì€?")
print("â±ï¸ Latency ë¶„í•´:")
for step, ms in timings.items():
    pct = ms / timings['total'] * 100
    print(f"  {step}: {ms:.0f}ms ({pct:.1f}%)")
```

### 6.3 í† í°/ë¹„ìš© ìµœì í™”

```python
# í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¡œ í† í° ì ˆê°

import tiktoken

enc = tiktoken.encoding_for_model("gpt-4o-mini")

# ê¸°ì¡´ í”„ë¡¬í”„íŠ¸
original_tokens = len(enc.encode(prompts.DETECTION_SUMMARY_V1))

# ìµœì í™” í”„ë¡¬í”„íŠ¸ (ë” ê°„ê²°í•˜ê²Œ)
optimized_prompt = """íƒì§€ê²°ê³¼: {model_name}, ê°€ì§œí™•ë¥  {fake_probability:.0%}, {risk_level}
ìš”ì²­: 1) ìœ„í—˜ ì´ìœ  2) íƒì§€ ê·¼ê±° 3) ê¶Œì¥ ì¡°ì¹˜ë¥¼ ê°„ê²°íˆ ì„¤ëª…"""

optimized_tokens = len(enc.encode(optimized_prompt))

print(f"í† í° ì ˆê°: {original_tokens} â†’ {optimized_tokens} ({(1-optimized_tokens/original_tokens)*100:.1f}% ì ˆê°)")
```

### 6.4 RAG í’ˆì§ˆ í‰ê°€

```python
# LLM-as-Judge í‰ê°€

evaluation_criteria = """
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-5ì  í‰ê°€:
1. ë²•ë ¹ ì¸ìš© ì •í™•ì„±: ì¡°í•­ ë²ˆí˜¸ê°€ ì •í™•í•œê°€?
2. ë‹µë³€ ê´€ë ¨ì„±: ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µí–ˆëŠ”ê°€?
3. ì‹¤ìš©ì„±: êµ¬ì²´ì  í–‰ë™ ê°€ì´ë“œë¥¼ ì œê³µí•˜ëŠ”ê°€?
4. ëª…í™•ì„±: ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€?
"""

def evaluate_rag_response(question, response, sources):
    """LLMìœ¼ë¡œ RAG ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    eval_prompt = f"""
{evaluation_criteria}

ì§ˆë¬¸: {question}
ë‹µë³€: {response}
ì¸ìš© ì¶œì²˜: {sources}

ê° ê¸°ì¤€ë³„ ì ìˆ˜ì™€ ì´ìœ ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
"""
    # LLM í˜¸ì¶œí•˜ì—¬ í‰ê°€
    result = call_llm(eval_prompt)
    return parse_evaluation(result)

# í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
test_cases = [
    ("ë³´ì´ìŠ¤í”¼ì‹± ì‹ ê³  ë°©ë²•", "HIGH"),
    ("í”¼í•´ê¸ˆ í™˜ê¸‰ ì ˆì°¨", "HIGH"),
    ("ì‚¬ê¸°ì£„ í˜•ëŸ‰", "MEDIUM"),
]

scores = []
for question, risk_level in test_cases:
    response = rag.query(question, risk_level)
    eval_result = evaluate_rag_response(question, response.answer, response.sources)
    scores.append(eval_result)

avg_score = sum(s['total'] for s in scores) / len(scores)
print(f"ğŸ“Š RAG í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_score:.2f}/5.0")
```

### âœ… Phase 6 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ëª¨ë¸ë³„ EER/min-tDCF ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ
- [ ] Latency ë³‘ëª© êµ¬ê°„ ì‹ë³„ ë° ê°œì„ 
- [ ] í† í° ì‚¬ìš©ëŸ‰ 10% ì´ìƒ ì ˆê°
- [ ] RAG í’ˆì§ˆ í‰ê°€ (LLM-as-Judge) ì‹¤í–‰
- [ ] ë²•ë ¹ ì¸ìš©ë¥  100% í™•ì¸

---

## Phase 7: ë°°í¬ ë° ë¬¸ì„œí™” (1-2ì¼)

### 7.1 Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile ìƒì„±
FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì†ŒìŠ¤ ì½”ë“œ
COPY src/ src/
COPY frontend/ frontend/
COPY data/legal_docs/ data/legal_docs/

# í™˜ê²½ ë³€ìˆ˜
ENV PYTHONPATH=/app

# í¬íŠ¸
EXPOSE 8000 8501

# ì‹¤í–‰
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 7.2 Docker Compose

```yaml
# docker-compose.yml ìƒì„±
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./models/checkpoints:/app/models/checkpoints
      - ./data/vectors:/app/data/vectors
    
  frontend:
    build: .
    command: streamlit run frontend/app.py --server.port 8501
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8000
    depends_on:
      - api
```

### 7.3 ë°°í¬ ì‹¤í–‰

```bash
# Docker ë¹Œë“œ ë° ì‹¤í–‰
docker-compose up --build -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì¤‘ì§€
docker-compose down
```

### 7.4 ìµœì¢… ë¬¸ì„œ ì ê²€

```bash
# ë¬¸ì„œ ëª©ë¡ í™•ì¸
ls -la *.md

# README.md - í”„ë¡œì íŠ¸ ê°œìš”
# MANUAL.md - ì´ ë§¤ë‰´ì–¼
```

### âœ… Phase 7 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Dockerfile ì‘ì„± ë° ë¹Œë“œ ì„±ê³µ
- [ ] Docker Compose ì„¤ì • ì™„ë£Œ
- [ ] ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
- [ ] README.md ìµœì¢… ì—…ë°ì´íŠ¸
- [ ] í”„ë¡œì íŠ¸ GitHub ì—…ë¡œë“œ (ì„ íƒ)

---

## ğŸ“Š ìµœì¢… ì„±ê³¼ ëª©í‘œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### íƒì§€ ì„±ëŠ¥
- [ ] ASVspoof 2021-LA EER < 5%
- [ ] min-tDCF ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ 
- [ ] Real-Time Factor < 1.0 (ì‹¤ì‹œê°„ ì²˜ë¦¬)

### ìš´ì˜ ìµœì í™”
- [ ] í‰ê·  ì‘ë‹µ Latency < 2ì´ˆ
- [ ] í† í° ë¹„ìš© 10% ì ˆê°
- [ ] ì—ëŸ¬ìœ¨ < 1%

### RAG í’ˆì§ˆ
- [ ] ë²•ë ¹ ì¸ìš©ë¥  100%
- [ ] LLM-as-Judge í‰ê·  4.0/5.0 ì´ìƒ

### ëª¨ë‹ˆí„°ë§
- [ ] Langfuse ëŒ€ì‹œë³´ë“œ í™œì„±í™”
- [ ] í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

---

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
# src/config.pyì—ì„œ DETECTION_BATCH_SIZE = 4 ë¡œ ë³€ê²½
```

### API ì—°ê²° ì˜¤ë¥˜
```bash
# í¬íŠ¸ í™•ì¸
netstat -tlnp | grep 8000

# ë°©í™”ë²½ í™•ì¸
sudo ufw allow 8000
```

### OpenAI API ì˜¤ë¥˜
```bash
# API í‚¤ í™•ì¸
echo $OPENAI_API_KEY

# ì”ì•¡ í™•ì¸
# https://platform.openai.com/usage
```

---

## ğŸ“ ì°¸ê³  ìë£Œ

- [ASVspoof Challenge](https://www.asvspoof.org/)
- [Langfuse Documentation](https://langfuse.com/docs)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°](https://www.law.go.kr/)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025ë…„ 12ì›” 1ì¼
